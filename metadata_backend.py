# -*- coding: utf-8 -*-
"""MARS_open_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v1v7o6-yEy7SF5UlIFp9sPan4B9K9umu
"""

!pip install pymupdf

from google.colab import files

uploaded = files.upload()
# After upload, you can access the file via list(uploaded.keys())[0]

# Step 2: Get file name from uploaded dictionary
file_name = list(uploaded.keys())[0]

# Step 3: Function to extract text and page count
def extract_text_from_pdf(file_path):
    doc = fitz.open(file_path)
    full_text = ""

    for page in doc:
        full_text += page.get_text()

    return {
        "text": full_text.strip(),
        "page_count": len(doc)
    }

# Step 4: Extract text from the uploaded PDF
result = extract_text_from_pdf(file_name)

# Step 5: Print results
print("Total Pages:", result["page_count"])
print("\n--- Sample Extracted Text ---\n")
print(result["text"][:1000])  # First 1000 characters

!sudo apt install tesseract-ocr
!pip install pytesseract Pillow

import fitz  # PyMuPDF
import pytesseract
from PIL import Image
import io

# OCR fallback function
def extract_text_with_ocr(page):
    # Render page to PNG image
    pix = page.get_pixmap(dpi=300)
    img_bytes = pix.tobytes("png")
    img = Image.open(io.BytesIO(img_bytes))

    # Run OCR
    return pytesseract.image_to_string(img)

# Final combined extractor with OCR fallback
def extract_text_from_pdf_with_ocr(file_path):
    doc = fitz.open(file_path)
    full_text = ""

    for page in doc:
        text = page.get_text().strip()

        if not text:
            print("Empty text found, running OCR...")
            text = extract_text_with_ocr(page)

        full_text += text + "\n\n"

    return {
        "text": full_text.strip(),
        "page_count": len(doc)
    }

file_name = list(uploaded.keys())[0]
result = extract_text_from_pdf_with_ocr(file_name)

print("Total Pages:", result["page_count"])
print("\n--- Sample OCR-backed Text ---\n")
print(result["text"][:1000])

from google.colab import files
uploaded = files.upload()

file_name = list(uploaded.keys())[0]

result = extract_text_from_pdf_with_ocr(file_name)

print("Total Pages:", result["page_count"])
print("\n--- Extracted Text ---\n")
print(result["text"][:1000])

from google.colab import files
uploaded = files.upload()

file_name = list(uploaded.keys())[0]

result = extract_text_from_pdf_with_ocr(file_name)

print("Total Pages:", result["page_count"])
print("\n--- Extracted Text ---\n")
print(result["text"][:1000])

!pip install langdetect

from langdetect import detect

def generate_basic_metadata(text, page_count):
    # Title: take first non-empty line (usually a heading)
    lines = text.split('\n')
    title = next((line.strip() for line in lines if line.strip()), "Unknown Title")

    # Language detection
    language = detect(text)

    return {
        "title": title,
        "language": language,
        "page_count": page_count
    }

# Example usage
metadata = generate_basic_metadata(result["text"], result["page_count"])
print(metadata)

from google.colab import files
uploaded = files.upload()

file_name = list(uploaded.keys())[0]

from langdetect import detect

def generate_basic_metadata(text, page_count):
    # Title: take first non-empty line (usually a heading)
    lines = text.split('\n')
    title = next((line.strip() for line in lines if line.strip()), "Unknown Title")

    # Language detection
    language = detect(text)

    return {
        "title": title,
        "language": language,
        "page_count": page_count
    }

# Example usage
metadata = generate_basic_metadata(result["text"], result["page_count"])
print(metadata)

result = extract_text_from_pdf_with_ocr(file_name)

print("Total Pages:", result["page_count"])
print("\n--- Extracted Text ---\n")
print(result["text"][:1000])

from langdetect import detect

def generate_basic_metadata(text, page_count):
    # Title: take first non-empty line (usually a heading)
    lines = text.split('\n')
    title = next((line.strip() for line in lines if line.strip()), "Unknown Title")

    # Language detection
    language = detect(text)

    return {
        "title": title,
        "language": language,
        "page_count": page_count
    }

# Example usage
metadata = generate_basic_metadata(result["text"], result["page_count"])
print(metadata)

from google.colab import files
uploaded = files.upload()

file_name = list(uploaded.keys())[0]

result = extract_text_from_pdf_with_ocr(file_name)

print("Total Pages:", result["page_count"])
print("\n--- Extracted Text ---\n")
print(result["text"][:1000])

from langdetect import detect

def generate_basic_metadata(text, page_count):
    # Title: take first non-empty line (usually a heading)
    lines = text.split('\n')
    title = next((line.strip() for line in lines if line.strip()), "Unknown Title")

    # Language detection
    language = detect(text)

    return {
        "title": title,
        "language": language,
        "page_count": page_count
    }

# Example usage
metadata = generate_basic_metadata(result["text"], result["page_count"])
print(metadata)

!pip install transformers
!pip install sentencepiece

from transformers import pipeline

# Load model
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Function to summarize
def generate_summary(text, max_input_len=1024):
    # Trim long input
    input_text = text[:max_input_len]

    # Generate summary
    summary = summarizer(input_text, max_length=100, min_length=30, do_sample=False)
    return summary[0]['summary_text']

summary = generate_summary(result["text"])
print("Summary:\n", summary)

!pip install keybert
!pip install sentence-transformers

from keybert import KeyBERT

# Load KeyBERT with default model
kw_model = KeyBERT()

# Extract keywords
def extract_keywords(text, top_n=10):
    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=top_n)
    return [kw[0] for kw in keywords]

keywords = extract_keywords(result["text"])
print("Keywords:", keywords)

!pip install spacy
!python -m spacy download en_core_web_sm

import spacy

# Load English NER model
import spacy
try:
    nlp = spacy.load("en_core_web_sm")
except:
    import os
    os.system("python -m spacy download en_core_web_sm")
    nlp = spacy.load("en_core_web_sm")


def extract_entities(text):
    doc = nlp(text)
    entities = {}

    for ent in doc.ents:
        label = ent.label_
        if label not in entities:
            entities[label] = set()
        entities[label].add(ent.text)

    # Convert sets to lists
    for label in entities:
        entities[label] = list(entities[label])

    return entities

entities = extract_entities(result["text"])
print("Named Entities:\n", entities)

!pip install transformers

from transformers import pipeline

# Load the zero-shot classification model
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

# Define candidate categories (you can tweak this list)
CATEGORIES = ["Scientific", "Technical", "Educational", "Legal", "Personal", "Business", "Government"]

def classify_document_category(text):
    input_text = text[:1000]  # limit to first part of doc
    result = classifier(input_text, candidate_labels=CATEGORIES)
    return result['labels'][0]  # highest scoring category

category = classify_document_category(result["text"])
print("Document Category:", category)

CATEGORIES = ["AI/ML", "Web Development", "Academic", "Research", "Technical", "Official"]

category = classify_document_category(result["text"])
print("Document Category:", category)

CATEGORIES = [
    "AI/ML", "Web Development", "Cybersecurity", "Healthcare", "Education",
    "Research", "Legal", "Finance", "Resume", "Business", "Marketing",
    "Engineering", "Government", "Technical", "Travel", "Personal", "HR", "Entertainment"
]

def generate_all_metadata(text, page_count):
    # Title, Language, Page Count
    basic = generate_basic_metadata(text, page_count)

    # Summary
    summary = generate_summary(text)

    # Keywords
    keywords = extract_keywords(text)

    # Entities
    entities = extract_entities(text)

    # Category (with expanded label list)
    categories = [
        "AI/ML", "Web Development", "Cybersecurity", "Healthcare", "Education",
        "Research", "Legal", "Finance", "Resume", "Business", "Marketing",
        "Engineering", "Government", "Technical", "Travel", "Personal", "HR", "Entertainment"
    ]
    category = classifier(text[:1000], candidate_labels=categories)['labels'][0]

    # Final metadata dictionary
    metadata = {
        "title": basic["title"],
        "language": basic["language"],
        "page_count": basic["page_count"],
        "summary": summary,
        "keywords": keywords,
        "entities": entities,
        "category": category
    }

    return metadata

final_metadata = generate_all_metadata(result["text"], result["page_count"])
import json
print(json.dumps(final_metadata, indent=2))

